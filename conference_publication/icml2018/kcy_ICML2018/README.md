# Representation Learning on Graphs with Jumping Knowledge Networks
- **author**: Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka
- **abstract**: Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.
- **keywords**: 
- **interpretation**: [csnd](
https://blog.csdn.net/geek_wh2016/article/details/81297106)
- **pdf**: [paper](http://proceedings.mlr.press/v80/xu18c/xu18c.pdf)
- **code**: [code](https://github.com/ShinKyuY/Representation_Learning_on_Graphs_with_Jumping_Knowledge_Networks)
- **dataset**: Citeseer, Cora, Reddit, PPI
- **ppt/video**:
- **curator**: Mengya Ji
